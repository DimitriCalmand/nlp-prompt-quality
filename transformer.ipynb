{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# On charge directement le split train/validation/test\n",
    "dataset = load_dataset(\"ibm-research/argument_quality_ranking_30k\", \"argument_quality_ranking\")\n",
    "\n",
    "# Aperçu rapide\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ],
   "id": "990a3db89840c9ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Fonction de tokenisation\n",
    "def preprocess(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"argument\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Appliquez la tokenisation sans retirer toutes les colonnes\n",
    "tokenized = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    # ne retirez QUE les colonnes texte brutes inutiles,\n",
    "    # mais conservez MACE-P\n",
    "    remove_columns=[\"argument\", \"topic\", \"set\", \"WA\", \"stance_WA\", \"stance_WA_conf\"]\n",
    ")"
   ],
   "id": "eb4672e3ec9b2718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def to_tf_dataset(split):\n",
    "    # Features: input_ids, attention_mask → X ; label: MACE-P → y\n",
    "    features = {\n",
    "        \"input_ids\": tf.constant(split[\"input_ids\"], dtype=tf.int32),\n",
    "        \"attention_mask\": tf.constant(split[\"attention_mask\"], dtype=tf.int32),\n",
    "    }\n",
    "    labels = tf.constant(split[\"MACE-P\"], dtype=tf.float32)\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "train_ds = to_tf_dataset(tokenized[\"train\"]).shuffle(10_000).batch(32)\n",
    "val_ds   = to_tf_dataset(tokenized[\"validation\"]).batch(32)\n",
    "test_ds  = to_tf_dataset(tokenized[\"test\"]).batch(32)"
   ],
   "id": "6dda7b9cfff67841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule, CosineDecayRestarts\n",
    "\n",
    "# 1. CosineDecayRestarts pour snapshots et redémarrages\n",
    "decay_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate=3e-5,\n",
    "    first_decay_steps=1000,   # ajuster selon nb_steps\n",
    "    t_mul=2.0,\n",
    "    m_mul=1.0,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "# 2. Classe WarmUp pour une montée linéaire des LR\n",
    "class WarmUp(LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, decay_schedule_fn, warmup_steps):\n",
    "        super().__init__()\n",
    "        # Conservez les valeurs pour la configuration\n",
    "        self.initial_lr = float(initial_lr)\n",
    "        self.warmup_steps = int(warmup_steps)\n",
    "        self.decay_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_float = tf.cast(step, tf.float32)\n",
    "        warmup_lr = tf.cast(self.initial_lr, tf.float32) * (step_float / tf.cast(self.warmup_steps, tf.float32))\n",
    "        decay_lr = self.decay_fn(step - self.warmup_steps)\n",
    "        return tf.where(step_float < self.warmup_steps, warmup_lr, decay_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Nécessaire pour la sérialisation du scheduler\n",
    "        return {\n",
    "            \"initial_lr\": self.initial_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_schedule_fn\": tf.keras.optimizers.schedules.deserialize(\n",
    "                {\n",
    "                    \"class_name\": self.decay_fn.__class__.__name__,\n",
    "                    \"config\": self.decay_fn.get_config()\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "\n",
    "lr_schedule = WarmUp(\n",
    "    initial_lr=3e-5,\n",
    "    decay_schedule_fn=decay_schedule,\n",
    "    warmup_steps=500\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ],
   "id": "2bf1e9081994ad2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        pos = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        i   = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle = pos / tf.pow(10000.0, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        pe = tf.where(tf.cast(i, tf.int32) % 2 == 0, tf.sin(angle), tf.cos(angle))\n",
    "        self.pe = pe[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pe[:, :tf.shape(x)[1], :]\n",
    "\n",
    "def build_transformer_model(\n",
    "    vocab_size: int,\n",
    "    max_len: int = 128,\n",
    "    d_model: int = 128,\n",
    "    num_heads: int = 4,\n",
    "    ff_dim: int = 256,\n",
    "    num_layers: int = 2,\n",
    "):\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    att_mask  = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    x = layers.Embedding(vocab_size, d_model)(input_ids)\n",
    "    x = layers.Dropout(0.1)(x)                       # dropout sur embeddings\n",
    "\n",
    "    x = PositionalEncoding(max_len, d_model)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-head Self-Attention\n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=0.1\n",
    "        )(x, x, attention_mask=att_mask[:, tf.newaxis, tf.newaxis, :])\n",
    "        attn_output = layers.Dropout(0.1)(attn_output)  # dropout sur attention\n",
    "        attn_output = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff = layers.Dense(ff_dim, activation=\"relu\")(attn_output)\n",
    "        ff = layers.Dropout(0.1)(ff)                   # dropout feed-forward\n",
    "        ff = layers.Dense(d_model)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(attn_output + ff)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    output = layers.Dense(1, activation=\"linear\", name=\"mace_p\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_ids, att_mask], outputs=output)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size  # supposant que tokenizer est déjà chargé\n",
    "model = build_transformer_model(vocab_size)\n",
    "model.summary()"
   ],
   "id": "a9f34d71344cda3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class UnfreezeCallback(Callback):\n",
    "    def __init__(self, freeze_epochs=2):\n",
    "        super().__init__()\n",
    "        self.freeze_epochs = freeze_epochs\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # geler embeddings et première couche d'attention\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Embedding) or 'multi_head_attention' in layer.name:\n",
    "                layer.trainable = False\n",
    "        print(\"Couches initiales gelées\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch + 1 == self.freeze_epochs:\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            print(f\"Couches dé-gelées à l'époque {epoch+1}\")\n",
    "\n",
    "unfreeze_cb = UnfreezeCallback(freeze_epochs=2)"
   ],
   "id": "cbd3e19fa500a75d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,                            # scheduler + Adam\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"MAE\")]\n",
    ")\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor=\"val_MAE\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_MAE\",\n",
    "    patience=3,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_cb, earlystop_cb, unfreeze_cb]"
   ],
   "id": "22a184c265d1a600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "id": "1f6c222db3853a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = model.evaluate(test_ds)\n",
    "print(f\"Test MAE: {result[1]:.4f}\")"
   ],
   "id": "ce8f6e822798eed7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
