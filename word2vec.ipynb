{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitri/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "splits = {'train': 'train.csv', 'validation': 'dev.csv', 'test': 'test.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/ibm-research/argument_quality_ranking_30k/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social media platforms have fueled the spread ...</td>\n",
       "      <td>Social media is harmful to society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online networks encourage shallow relationship...</td>\n",
       "      <td>Social media is harmful to society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The addictive design of social media steals ti...</td>\n",
       "      <td>Social media is harmful to society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social media allows people to connect across v...</td>\n",
       "      <td>Social media is harmful to society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These platforms can be powerful tools for educ...</td>\n",
       "      <td>Social media is harmful to society</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            argument  \\\n",
       "0  Social media platforms have fueled the spread ...   \n",
       "1  Online networks encourage shallow relationship...   \n",
       "2  The addictive design of social media steals ti...   \n",
       "3  Social media allows people to connect across v...   \n",
       "4  These platforms can be powerful tools for educ...   \n",
       "\n",
       "                                topic  \n",
       "0  Social media is harmful to society  \n",
       "1  Social media is harmful to society  \n",
       "2  Social media is harmful to society  \n",
       "3  Social media is harmful to society  \n",
       "4  Social media is harmful to society  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "\n",
    "with open(\"./datasets_chatgpt.csv\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read().replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "df_augmented_chat_gpt = pd.read_csv(\n",
    "    io.StringIO(raw),\n",
    "    sep=r',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)',\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "for col in [\"argument\", \"topic\"]:\n",
    "    df_augmented_chat_gpt[col] = (\n",
    "        df_augmented_chat_gpt[col]\n",
    "        .str.strip('\"')\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "# Vérification\n",
    "df_augmented_chat_gpt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument = list(df.argument) + list(df_augmented_chat_gpt.argument)\n",
    "topic = list(df.topic) + list(df_augmented_chat_gpt.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dimitri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de vocabulaire : ['<PAD>', 'should', 'we', '.', 'the', 'to', 'and', 'of', 'a', 'be']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "PADDINGS_TOPICS = 10\n",
    "PADDINGS_ARGUMENTS = 100\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = []\n",
    "for i in range(len(argument)):\n",
    "    arg_token = word_tokenize(argument[i].lower())\n",
    "    arg_topic = word_tokenize(topic[i].lower())\n",
    "    if len(arg_topic) < PADDINGS_TOPICS:\n",
    "        arg_topic += ['<PAD>'] * (PADDINGS_TOPICS - len(arg_topic))\n",
    "    if len(arg_token) < PADDINGS_ARGUMENTS:\n",
    "        arg_token += ['<PAD>'] * (PADDINGS_ARGUMENTS - len(arg_token))\n",
    "\n",
    "    data.append(arg_topic + arg_token)\n",
    "\n",
    "model = Word2Vec(sentences=data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(\"Exemple de vocabulaire :\", list(model.wv.key_to_index.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.Series([\"What\"])\n",
    "\n",
    "# Tokenisation de chaque phrase en minuscules\n",
    "test_sentences = test_data.apply(lambda x: word_tokenize(x.lower())).tolist()\n",
    "\n",
    "# Fonction pour calculer un vecteur moyen pour une phrase\n",
    "def sentence_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return None\n",
    "\n",
    "# Calculer le vecteur moyen pour chaque phrase du dataset Test\n",
    "test_vectors = [sentence_vector(sentence, model) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots les plus similaires au vecteur de la phrase Test : [('what', 1.0000001192092896), ('whatever', 0.7845554947853088), ('how', 0.7578881978988647), ('going', 0.7202796936035156), ('something', 0.7185078263282776), ('nobody', 0.6992220282554626), ('everything', 0.6940007209777832), ('someone', 0.6769415140151978), ('passionate', 0.659133791923523), ('th', 0.6571049094200134)]\n"
     ]
    }
   ],
   "source": [
    "def decode_embedding(embedding, model, topn=10):\n",
    "    return model.wv.similar_by_vector(embedding, topn=topn)\n",
    "\n",
    "# Décoder le vecteur moyen de la première phrase du dataset Test\n",
    "decoded_words = decode_embedding(test_vectors[0], model)\n",
    "\n",
    "# Affichage des mots les plus similaires\n",
    "print(\"Mots les plus similaires au vecteur de la phrase Test :\", decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms(word, model, topn=10):\n",
    "    \"\"\"\n",
    "    Find synonyms for a given word using the Word2Vec model.\n",
    "    \"\"\"\n",
    "    list_syno = decode_embedding(model.wv[word], model, topn)\n",
    "    return list_syno[1][0]\n",
    "def sentence_synonyms(sentence, model, topn=10):\n",
    "    \"\"\"\n",
    "    Find synonyms for each word in a sentence using the Word2Vec model.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    sentence = sentence.lower().split()\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            res.append(synonyms(word, model, topn))\n",
    "        else:\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whatever', 'perfect', 'were', 'they']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_synonyms(\"how old are you\", model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
