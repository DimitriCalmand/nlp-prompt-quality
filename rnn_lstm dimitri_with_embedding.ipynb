{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitri/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## train test split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_missing = 3\n",
    "MODEL_PATH = \"rnn_model.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\"\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prompts_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.Prompt\n",
    "note = df.Complexite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM\n",
    "\n",
    "# Charger le tokenizer et le modèle BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de données\n",
    "texts = list(data)[:1000]\n",
    "\n",
    "# Tokenisation et encodage des textes\n",
    "encoded_inputs = tokenizer(texts, padding=True, truncation=True, max_length=MAX_SEQUENCE_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "# Obtenir les embeddings de BERT\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**encoded_inputs)\n",
    "    # Utiliser les embeddings de la dernière couche cachée\n",
    "    bert_embeddings = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Convertir les embeddings en numpy pour Keras\n",
    "bert_embeddings = bert_embeddings.numpy()\n",
    "\n",
    "# Entraîner le modèle (avec des données d'entraînement adaptées)\n",
    "# model.fit(bert_embeddings_train, y_train, validation_data=(bert_embeddings_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 68, 768)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, np.array(list(note)[:1000]), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire le modèle LSTM avec les embeddings de BERT\n",
    "model = Sequential()\n",
    "\n",
    "# Ajouter des couches bidirectionnelles LSTM\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, input_shape=(MAX_SEQUENCE_LENGTH, bert_embeddings.shape[-1]))))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "# Ajouter des couches denses\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:3229\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m \n\u001b[1;32m   3200\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3233\u001b[0m     )\n\u001b[1;32m   3234\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3236\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3242\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 7s 142ms/step - loss: 1.1105 - val_loss: 0.8795\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 2s 96ms/step - loss: 0.8106 - val_loss: 0.7984\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 2s 96ms/step - loss: 0.6988 - val_loss: 0.7998\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 2s 100ms/step - loss: 0.6396 - val_loss: 0.7282\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 2s 100ms/step - loss: 0.6274 - val_loss: 0.7271\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 102ms/step - loss: 0.5749 - val_loss: 0.7361\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 3s 102ms/step - loss: 0.5503 - val_loss: 0.7443\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 106ms/step - loss: 0.5021 - val_loss: 0.7472\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 3s 107ms/step - loss: 0.4449 - val_loss: 0.7721\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 108ms/step - loss: 0.4464 - val_loss: 0.7189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x70ecf9005330>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate model with train and test\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    data = [data]\n",
    "    # Tokenisation et encodage des textes\n",
    "    encoded_inputs = tokenizer(data, padding=True, truncation=True, max_length=MAX_SEQUENCE_LENGTH, return_tensors=\"pt\")\n",
    "    # Obtenir les embeddings de BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded_inputs)\n",
    "        # Utiliser les embeddings de la dernière couche cachée\n",
    "        bert_embeddings = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    # Convertir les embeddings en numpy pour Keras\n",
    "    bert_embeddings = bert_embeddings.numpy()\n",
    "    print(bert_embeddings.shape)\n",
    "    prediction = model.predict(bert_embeddings)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 768)\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.9818515]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Un éléphant pèse 5 000 kg et tient sur une patte. Quel est le poids supporté par cette patte ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
